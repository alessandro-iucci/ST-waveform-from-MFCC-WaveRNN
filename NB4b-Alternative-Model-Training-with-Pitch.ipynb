{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Model (Training)\n",
    "\n",
    "I've found WaveRNN quite slow to train so here's an alternative that utilises the optimised rnn kernels in Pytorch. The model below is much much faster to train, it will converge in 48hrs when training on 22.5kHz samples (or 24hrs using 16kHz samples) on a single GTX1080. It also works quite well with predicted GTA features. \n",
    "\n",
    "The model is simply two residual GRUs in sequence and then three dense layers with a 512 softmax output. This is supplemented with an upsampling network.\n",
    "\n",
    "Since the Pytorch rnn kernels are 'closed', the options for conditioning sites are greatly reduced. Here's the strategy I went with given that restriction:  \n",
    "\n",
    "1 - Upsampling: Nearest neighbour upsampling followed by 2d convolutions with 'horizontal' kernels to interpolate. Split up into two or three layers depending on the stft hop length.\n",
    "\n",
    "2 - A 1d resnet with a 5 wide conv input and 1x1 res blocks. Not sure if this is necessary, but the thinking behind it is: the upsampled features give a local view of the conditioning - why not supplement that with a much wider view of conditioning features, including a peek at the future. One thing to note is that the resnet is computed only once and in parallel, so it shouldn't slow down training/generation much. \n",
    "\n",
    "Train this model to ~500k steps for 8/9bit linear samples or ~1M steps for 10bit linear or 9+bit mu_law. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math, pickle, os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.display import *\n",
    "from utils.dsp import *\n",
    "import python_speech_features as psf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1053"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bits = 9\n",
    "pad = 2\n",
    "seq_len = hop_length * 9\n",
    "#9 so that the kernel outputs the right size\n",
    "seq_len\n",
    "#seq_len = coef * mel_win\n",
    "#coef = total_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_name = 'nb4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sintassi del comando errata.\n"
     ]
    }
   ],
   "source": [
    "%mkdir -p 'model_checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sintassi del comando errata.\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = f'model_checkpoints/{notebook_name}.pyt'\n",
    "DATA_PATH = f'data/'\n",
    "STEP_PATH = f'model_checkpoints/{notebook_name}_step.npy'\n",
    "GEN_PATH = f'model_outputs/{notebook_name}/'\n",
    "%mkdir -p $GEN_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATA_PATH}dataset_ids.pkl', 'rb') as f:\n",
    "    dataset_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = dataset_ids[:-2048]\n",
    "dataset_ids = dataset_ids[-2048:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class AudiobookDataset(Dataset):\n",
    "    def __init__(self, ids, path):\n",
    "        self.path = path\n",
    "        self.metadata = ids\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        file = self.metadata[index]\n",
    "        m = np.load(f'{self.path}mel/{file}.npy')\n",
    "        x = np.load(f'{self.path}quant/{file}.npy')\n",
    "        pitch = np.load(f'{self.path}pitch/{file}.npy')\n",
    "        return m, x, pitch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch) :\n",
    "    #mel_win = seq_len // hop_length + 2 * pad\n",
    "    s=0\n",
    "    for x in batch:\n",
    "        s+=x[0].shape[-1]/len(x[1])\n",
    "    proportional_coeff = s/len(batch)\n",
    "    \n",
    "    mel_win = round(seq_len * proportional_coeff)\n",
    "    #print(mel_win)\n",
    "    max_offsets = [x[1].shape[-1] - (seq_len + 2 * pad) for x in batch]\n",
    "    sig_offsets = [np.random.randint(0, offset) for offset in max_offsets]\n",
    "    mel_offsets = [round(offset*proportional_coeff) for i,offset in enumerate(sig_offsets)]\n",
    "\n",
    "    mels = [x[0][:, mel_offsets[i]:mel_offsets[i] + mel_win] \\\n",
    "            for i, x in enumerate(batch)]\n",
    "    \n",
    "    coarse = [x[1][sig_offsets[i]:sig_offsets[i] + seq_len + 1] \\\n",
    "              for i, x in enumerate(batch)]\n",
    "    \n",
    "    pitch = [x[2][sig_offsets[i]:sig_offsets[i] + seq_len + 1] \\\n",
    "              for i, x in enumerate(batch)]\n",
    "    \n",
    "    mels = np.stack(mels).astype(np.float32)\n",
    "    coarse = np.stack(coarse).astype(np.int64)\n",
    "    pitch = np.stack(pitch).astype(np.float32)\n",
    "   \n",
    "    \n",
    "    mels = torch.FloatTensor(mels)\n",
    "    coarse = torch.LongTensor(coarse)\n",
    "    pitch = torch.FloatTensor(pitch)\n",
    "    \n",
    "    x_input = 2 * coarse[:, :seq_len].float() / (2**bits - 1.) - 1.\n",
    "    \n",
    "    y_coarse = coarse[:, 1:]\n",
    "    \n",
    "    return x_input, mels, pitch[:, :seq_len], y_coarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudiobookDataset(dataset_ids, DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, collate_fn=collate, batch_size=32, \n",
    "                         num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1053]),\n",
       " torch.Size([32, 13, 9]),\n",
       " torch.Size([32, 1053]),\n",
       " torch.Size([32, 1053]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, m, p, y = next(iter(data_loader))\n",
    "x.shape, m.shape, p.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module) :\n",
    "    def __init__(self, dims) :\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(dims)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(dims)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelResNet(nn.Module) :\n",
    "    def __init__(self, res_blocks, in_dims, compute_dims, res_out_dims) :\n",
    "        super().__init__()\n",
    "        #k_size = pad * 2 + 1\n",
    "        k_size = 1\n",
    "        self.conv_in = nn.Conv1d(in_dims, compute_dims, kernel_size=k_size, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm1d(compute_dims)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(res_blocks) :\n",
    "            self.layers.append(ResBlock(compute_dims))\n",
    "        self.conv_out = nn.Conv1d(compute_dims, res_out_dims, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        #x: (32, 13, 9) = (batch_size, number_mel_coeff, mel_seq_len)\n",
    "        x = self.conv_in(x)\n",
    "        #x: (32, 128, 5) = (batch_size, compute_dims, kernel_size)\n",
    "        x = self.batch_norm(x)\n",
    "        #x: (32, 128, 5)\n",
    "        x = F.relu(x)\n",
    "        for f in self.layers : x = f(x)\n",
    "        #x: (32, 128, 5)\n",
    "        x = self.conv_out(x)\n",
    "        #x: (32, 128, 5)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stretch2d(nn.Module) :\n",
    "    def __init__(self, x_scale, y_scale) :\n",
    "        super().__init__()\n",
    "        self.x_scale = x_scale\n",
    "        self.y_scale = y_scale\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        b, c, h, w = x.size()\n",
    "        x = x.unsqueeze(-1).unsqueeze(3)\n",
    "        x = x.repeat(1, 1, 1, self.y_scale, 1, self.x_scale)\n",
    "        return x.view(b, c, h * self.y_scale, w * self.x_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleNetwork(nn.Module) :\n",
    "    def __init__(self, feat_dims, upsample_scales, compute_dims, \n",
    "                 res_blocks, res_out_dims, pad) :\n",
    "        \"\"\"\n",
    "        upsample_scales = (5,5,11)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        total_scale = np.cumproduct(upsample_scales)[-1]\n",
    "        self.indent = pad * total_scale\n",
    "        self.resnet = MelResNet(res_blocks, feat_dims, compute_dims, res_out_dims)\n",
    "        self.resnet_stretch = Stretch2d(total_scale, 1)\n",
    "        self.up_layers = nn.ModuleList()\n",
    "        for scale in upsample_scales :\n",
    "            k_size = (1, scale * 2 + 1)\n",
    "            padding = (0, scale)\n",
    "            stretch = Stretch2d(scale, 1)\n",
    "            conv = nn.Conv2d(1, 1, kernel_size=k_size, padding=padding, bias=False)\n",
    "            conv.weight.data.fill_(1. / k_size[1])\n",
    "            self.up_layers.append(stretch)\n",
    "            self.up_layers.append(conv)\n",
    "    \n",
    "    def forward(self, m) :\n",
    "        #m: (32, 13, 5) = (batch_size, number_mel_coeff, mel_seq_len)\n",
    "        aux = self.resnet(m).unsqueeze(1)\n",
    "        #kernel_size = pad*2 + 1\n",
    "        #aux: (32, 1, 128, 5) = (batch_size, 1, compute_dims, kernel_size)\n",
    "        aux = self.resnet_stretch(aux)\n",
    "        #aux: (32, 1, 128, 585) = (batch_size, 1, compute_dims, total_scale*kernel_size)\n",
    "        aux = aux.squeeze(1)\n",
    "        #aux: (32, 128, 1375) = (batch_size, compute_dims, total_scale*kernel_size)\n",
    "        m = m.unsqueeze(1)\n",
    "        #m: (32, 1, 13, 9) = (batch_size, 1, number_mel_coeff, mel_seq_len)\n",
    "        for f in self.up_layers : \n",
    "            \"\"\"\n",
    "            1 stretch = Stretch2D(scale,1)\n",
    "            1 conv = Conv2D(1,1, kernel_size = (1,2*scale+1), padding= (0,scale)) (doesn't change the size)\n",
    "            \"\"\"\n",
    "            m = f(m)\n",
    "            #m: (32, 1, 13, 45) = (batch_size, 1, number_mel_coeff, mel_seq_len*5)\n",
    "            #m: (32, 1, 13, 225) = (batch_size, 1, number_mel_coeff, mel_seq_len*5*5)\n",
    "            #m: (32, 1, 13, 2475) = (batch_size, 1, number_mel_coeff, mel_seq_len*5*5*11)\n",
    "            \n",
    "        #m: (32, 1, 13, 2475) = (batch_size, 1, number_mel_coeff, mel_seq_len*total_scale)\n",
    "        #m = m.squeeze(1)[:, :, self.indent:-self.indent]\n",
    "        m=m.squeeze(1)\n",
    "        ##m: (32, 13, 1375) = (batch_size, number_mel_coeff, (mel_seq_len-2*pad)*total_scale)\n",
    "        #Where total_scale is the the product of the upsample_factors \n",
    "        return m.transpose(1, 2), aux.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module) :\n",
    "    def __init__(self, rnn_dims, fc_dims, bits, pad, upsample_factors,\n",
    "                 feat_dims, compute_dims, res_out_dims, res_blocks,\n",
    "                 hop_length, sample_rate):\n",
    "        super().__init__()\n",
    "        self.pad = pad\n",
    "        self.n_classes = 2**bits\n",
    "        self.rnn_dims = rnn_dims\n",
    "        self.aux_dims = res_out_dims // 4\n",
    "        self.hop_length = hop_length\n",
    "        self.sample_rate = sample_rate\n",
    "        self.upsample = UpsampleNetwork(feat_dims, upsample_factors, compute_dims, \n",
    "                                        res_blocks, res_out_dims, pad)\n",
    "        self.I = nn.Linear(feat_dims + self.aux_dims + 1, rnn_dims)\n",
    "        self.rnn1 = nn.GRU(rnn_dims, rnn_dims, batch_first=True)\n",
    "        self.rnn2 = nn.GRU(rnn_dims + self.aux_dims, rnn_dims, batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_dims + self.aux_dims, fc_dims)\n",
    "        self.fc2 = nn.Linear(fc_dims + self.aux_dims, fc_dims)\n",
    "        self.fc3 = nn.Linear(fc_dims, self.n_classes)\n",
    "        num_params(self)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, mels, pitch) :\n",
    "        \"\"\"\n",
    "        Input\n",
    "        x:  (32,1100) = (batch_size, seq_len)\n",
    "        mels: (32, 13, 9) = (batch_size, number_mel_coeff, mel_seq_len)\n",
    "        Output\n",
    "        y: ()\n",
    "        \n",
    "        \"\"\"\n",
    "        bsize = mels.size(0)\n",
    "        h1 = torch.zeros(1, bsize, self.rnn_dims).cuda()\n",
    "        h2 = torch.zeros(1, bsize, self.rnn_dims).cuda()\n",
    "        mels, aux = self.upsample(mels)\n",
    "        mels[:,:,-1] = pitch\n",
    "        \n",
    "        #total_scale is the product of the upsample_factors\n",
    "        #mels: (32, 1100, 13) = (batch_size, (mel_seq_len-2*pad)*total_scale, number_mel_coeff)\n",
    "        #aux: (32, 1100, 128) = (batch_size, total_scale*kernel_size, compute_dims)\n",
    "        \n",
    "        aux_idx = [self.aux_dims * i for i in range(5)]\n",
    "        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n",
    "        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n",
    "        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n",
    "        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n",
    "        #a_i: (32, 1100, 32) = (batch_size, total_scale*kernel_size, res_out_dims//4)\n",
    "        \n",
    "        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n",
    "        #in_seq_length = total_scale*kernel_size = (mel_seq_len-2*pad)*total_scale\n",
    "        #We want in_seq_length = 1100 (=seq_len)\n",
    "        # total_scale = 220 -> scale = (2,2,5,11)\n",
    "        #x: (32, 1100, 45) = (batch_size, in_seq_length, res_out_dims//4 + number_mel_coeff)\n",
    "        x = self.I(x)\n",
    "        #x: (32, 1100, 512) = (batch_size, in_seq_length, rnn_dims)\n",
    "        res = x\n",
    "        x, _ = self.rnn1(x, h1)\n",
    "        #x: (32, 1100, 512) = (batch_size, in_seq_length, rnn_dims)\n",
    "        \n",
    "        x = x + res\n",
    "        res = x\n",
    "        x = torch.cat([x, a2], dim=2)\n",
    "        x, _ = self.rnn2(x, h2)\n",
    "        \n",
    "        x = x + res\n",
    "        x = torch.cat([x, a3], dim=2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = torch.cat([x, a4], dim=2)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = F.log_softmax(self.fc3(x), dim=-1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def generate(self, mels, pitch, save_path, batched, target, overlap) :\n",
    "        \n",
    "        self.eval()\n",
    "        output = []\n",
    "        start = time.time()\n",
    "        rnn1 = self.get_gru_cell(self.rnn1)\n",
    "        rnn2 = self.get_gru_cell(self.rnn2)\n",
    "        \n",
    "        with torch.no_grad() :\n",
    "            \n",
    "            mels = torch.FloatTensor(mels).cuda().unsqueeze(0)\n",
    "            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side='both')\n",
    "            mels, aux = self.upsample(mels.transpose(1, 2))\n",
    "            pitch = torch.FloatTensor(pitch).cuda().unsqueeze(0).unsqueeze(2)\n",
    "            pitch = self.pad_tensor(pitch, pad=(mels.shape[1]-pitch.shape[1])//2,side='before')\n",
    "            pitch = self.pad_tensor(pitch, pad=mels.shape[1]-pitch.shape[1],side='after').transpose(1,2)\n",
    "            mels[:,:,-1] = pitch\n",
    "            \n",
    "            if batched :\n",
    "                mels = self.fold_with_overlap(mels, target, overlap)\n",
    "                aux = self.fold_with_overlap(aux, target, overlap)\n",
    "\n",
    "            b_size, seq_len, _ = mels.size()\n",
    "            \n",
    "            h1 = torch.zeros(b_size, self.rnn_dims).cuda()\n",
    "            h2 = torch.zeros(b_size, self.rnn_dims).cuda()\n",
    "            x = torch.zeros(b_size, 1).cuda()\n",
    "            \n",
    "            d = self.aux_dims\n",
    "            aux_split = [aux[:, :, d*i:d*(i+1)] for i in range(4)]\n",
    "            \n",
    "            for i in range(seq_len) :\n",
    "\n",
    "                m_t = mels[:, i, :]\n",
    "                \n",
    "                a1_t, a2_t, a3_t, a4_t = \\\n",
    "                    (a[:, i, :] for a in aux_split)\n",
    "                \n",
    "                x = torch.cat([x, m_t, a1_t], dim=1)\n",
    "                x = self.I(x)\n",
    "                h1 = rnn1(x, h1)\n",
    "                \n",
    "                x = x + h1\n",
    "                inp = torch.cat([x, a2_t], dim=1)\n",
    "                h2 = rnn2(inp, h2)\n",
    "                \n",
    "                x = x + h2\n",
    "                x = torch.cat([x, a3_t], dim=1)\n",
    "                x = F.relu(self.fc1(x))\n",
    "                \n",
    "                x = torch.cat([x, a4_t], dim=1)\n",
    "                x = F.relu(self.fc2(x))\n",
    "                \n",
    "                logits = self.fc3(x)\n",
    "                posterior = F.softmax(logits, dim=1)\n",
    "                distrib = torch.distributions.Categorical(posterior)\n",
    "                \n",
    "                sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n",
    "                output.append(sample)\n",
    "                x = sample.unsqueeze(-1)\n",
    "                \n",
    "                if i % 100 == 0 : self.gen_display(i, seq_len, b_size, start)\n",
    "                    \n",
    "        \n",
    "        output = torch.stack(output).transpose(0, 1)\n",
    "        output = output.cpu().numpy()\n",
    "        output = output.astype(np.float64)\n",
    "        \n",
    "        if batched :\n",
    "            output = self.xfade_and_unfold(output, target, overlap)\n",
    "        else :\n",
    "            output = output[0]\n",
    "            \n",
    "        librosa.output.write_wav(save_path, output.astype(np.float32), self.sample_rate)\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def gen_display(self, i, seq_len, b_size, start) :\n",
    "        gen_rate = (i + 1) / (time.time() - start) * b_size / 1000 \n",
    "        realtime_ratio = gen_rate * 1000 / self.sample_rate\n",
    "        stream('%i/%i -- batch_size: %i -- gen_rate: %.1f kHz -- x_realtime: %.1f  ', \n",
    "              (i * b_size, seq_len * b_size, b_size, gen_rate, realtime_ratio))\n",
    "    \n",
    "    def get_gru_cell(self, gru) :\n",
    "        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n",
    "        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n",
    "        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n",
    "        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n",
    "        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n",
    "        return gru_cell\n",
    "    \n",
    "    \n",
    "    def pad_tensor(self, x, pad, side='both') :\n",
    "        # NB - this is just a quick method i need right now\n",
    "        # i.e., it won't generalise to other shapes/dims\n",
    "        b, t, c = x.size()\n",
    "        total = t + 2 * pad if side == 'both' else t + pad\n",
    "        padded = torch.zeros(b, total, c).cuda()\n",
    "        if side == 'before' or side == 'both' :\n",
    "            padded[:, pad:pad+t, :] = x\n",
    "        elif side == 'after' :\n",
    "            padded[:, :t, :] = x    \n",
    "        return padded\n",
    "\n",
    "    \n",
    "    def fold_with_overlap(self, x, target, overlap) :\n",
    "        \n",
    "        ''' Fold the tensor with overlap for quick batched inference.\n",
    "            Overlap will be used for crossfading in xfade_and_unfold()\n",
    "\n",
    "        Args:\n",
    "            x (tensor)    : Upsampled conditioning features. \n",
    "                            shape=(1, timesteps, features)\n",
    "            target (int)  : Target timesteps for each index of batch\n",
    "            overlap (int) : Timesteps for both xfade and rnn warmup\n",
    "\n",
    "        Return:\n",
    "            (tensor) : shape=(num_folds, target + 2 * overlap, features)\n",
    "         \n",
    "        Details:      \n",
    "            x = [[h1, h2, ... hn]] \n",
    "\n",
    "            Where each h is a vector of conditioning features\n",
    "\n",
    "            Eg: target=2, overlap=1 with x.size(1)=10 \n",
    "\n",
    "            folded = [[h1, h2, h3, h4],\n",
    "                      [h4, h5, h6, h7],\n",
    "                      [h7, h8, h9, h10]]\n",
    "        '''\n",
    "\n",
    "        _, total_len, features = x.size()\n",
    "        \n",
    "        # Calculate variables needed\n",
    "        num_folds = (total_len - overlap) // (target + overlap)\n",
    "        extended_len = num_folds * (overlap + target) + overlap\n",
    "        remaining = total_len - extended_len\n",
    "        \n",
    "        # Pad if some time steps poking out\n",
    "        if remaining != 0 :\n",
    "            num_folds += 1\n",
    "            padding = target + 2 * overlap - remaining    \n",
    "            x = self.pad_tensor(x, padding, side='after')\n",
    "\n",
    "        folded = torch.zeros(num_folds, target + 2 * overlap, features).cuda()\n",
    "        \n",
    "        # Get the values for the folded tensor\n",
    "        for i in range(num_folds) :\n",
    "            start = i * (target + overlap)\n",
    "            end = start + target + 2 * overlap\n",
    "            folded[i] = x[:, start:end, :]\n",
    "\n",
    "        return folded\n",
    "    \n",
    "    \n",
    "    def xfade_and_unfold(self, y, target, overlap) :\n",
    "        \n",
    "        ''' Applies a crossfade and unfolds into a 1d array.\n",
    "            \n",
    "        Args:\n",
    "            y (ndarry)    : Batched sequences of audio samples\n",
    "                            shape=(num_folds, target + 2 * overlap)\n",
    "                            dtype=np.float64\n",
    "            overlap (int) : Timesteps for both xfade and rnn warmup\n",
    "\n",
    "        Return:\n",
    "            (ndarry) : audio samples in a 1d array  \n",
    "                       shape=(total_len)\n",
    "                       dtype=np.float64\n",
    "        \n",
    "        Details: \n",
    "            y = [[seq1], \n",
    "                 [seq2], \n",
    "                 [seq3]] \n",
    "            \n",
    "            Apply a gain envelope at both ends of the sequences\n",
    "        \n",
    "            y = [[seq1_in, seq1_target, seq1_out],\n",
    "                 [seq2_in, seq2_target, seq2_out],\n",
    "                 [seq3_in, seq3_target, seq3_out]]\n",
    "\n",
    "            Stagger and add up the groups of samples:\n",
    "\n",
    "            [seq1_in, seq1_target, (seq1_out + seq2_in), seq2_target, ...]\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        num_folds, length = y.shape\n",
    "        target = length - 2 * overlap\n",
    "        total_len = num_folds * (target + overlap) + overlap\n",
    "        \n",
    "        # Need some silence for the rnn warmup\n",
    "        silence_len = overlap // 2\n",
    "        fade_len = overlap - silence_len\n",
    "        silence = np.zeros((silence_len), dtype=np.float64)\n",
    "        \n",
    "        # Equal power crossfade\n",
    "        t = np.linspace(-1, 1, fade_len, dtype=np.float64)\n",
    "        fade_in = np.sqrt(0.5 * (1 + t))\n",
    "        fade_out = np.sqrt(0.5 * (1 - t))\n",
    "        \n",
    "        # Concat the silence to the fades\n",
    "        fade_in = np.concatenate([silence, fade_in])\n",
    "        fade_out = np.concatenate([fade_out, silence])\n",
    "        \n",
    "        # Apply the gain to the overlap samples\n",
    "        y[:, :overlap] *= fade_in\n",
    "        y[:, -overlap:] *= fade_out\n",
    "        \n",
    "        unfolded = np.zeros((total_len), dtype=np.float64)\n",
    "        \n",
    "        # Loop to add up all the samples\n",
    "        for i in range(num_folds ) :\n",
    "            start = i * (target + overlap)\n",
    "            end = start + target + 2 * overlap\n",
    "            unfolded[start:end] += y[i]\n",
    "\n",
    "        return unfolded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and Check Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters: 4.397 million\n"
     ]
    }
   ],
   "source": [
    "model = Model(rnn_dims=512, \n",
    "              fc_dims=512, \n",
    "              bits=bits,\n",
    "              pad=pad,\n",
    "              upsample_factors=(3, 3, 13), \n",
    "              feat_dims=13,\n",
    "              compute_dims=128, \n",
    "              res_out_dims=128, \n",
    "              res_blocks=10,\n",
    "              hop_length=hop_length,\n",
    "              sample_rate=sample_rate).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    torch.save(model.state_dict(), MODEL_PATH) \n",
    "model.load_state_dict(torch.load(MODEL_PATH)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mels, aux = model.upsample(m.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_spec(m[0].numpy())\n",
    "plot_spec(mels[0].cpu().detach().numpy().T)\n",
    "#print(mels.shape)\n",
    "#print(aux.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "global step\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3456)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(STEP_PATH):\n",
    "    np.save(STEP_PATH, step)\n",
    "step = np.load(STEP_PATH)\n",
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, epochs, batch_size, classes, seq_len, step, lr=1e-4) :\n",
    "        \n",
    "    for p in optimiser.param_groups : p['lr'] = lr\n",
    "    criterion = nn.NLLLoss().cuda()\n",
    "    \n",
    "    for e in range(epochs) :\n",
    "\n",
    "        trn_loader = DataLoader(dataset, collate_fn=collate, batch_size=batch_size, \n",
    "                                num_workers=0, shuffle=True, pin_memory=True)\n",
    "    \n",
    "        running_loss = 0.\n",
    "        val_loss = 0.\n",
    "        start = time.time()\n",
    "        running_loss = 0.\n",
    "\n",
    "        iters = len(trn_loader)\n",
    "\n",
    "        for i, (x, m, p, y) in enumerate(trn_loader) :\n",
    "            \n",
    "            x, m, p, y = x.cuda(), m.cuda(), p.cuda(), y.cuda()\n",
    "\n",
    "            y_hat = model(x, m, p)\n",
    "            y_hat = y_hat.transpose(1, 2).unsqueeze(-1)\n",
    "            y = y.unsqueeze(-1)\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            speed = (i + 1) / (time.time() - start)\n",
    "            avg_loss = running_loss / (i + 1)\n",
    "            \n",
    "            step += 1\n",
    "            k = step // 1000\n",
    "            stream('Epoch: %i/%i -- Batch: %i/%i -- Loss: %.3f -- %.2f steps/sec -- Step: %ik ', \n",
    "                   (e + 1, epochs, i + 1, iters, avg_loss, speed, k))\n",
    "        \n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        np.save(STEP_PATH, step)\n",
    "        print(' <saved>')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimiser = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "current_thousand = -1\n",
    "optimiser = optim.Adam(model.parameters())\n",
    "print(optimiser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate(samples=3, batched=True, target=11_000, overlap=550,plot_input=False,plot_output=True) :\n",
    "    outputs = []\n",
    "    k = step // 1000\n",
    "    test_mels = [np.load(f'{DATA_PATH}mel/{id}.npy') for id in test_ids[:samples]]\n",
    "    ground_truth = [np.load(f'{DATA_PATH}quant/{id}.npy') for id in test_ids[:samples]]\n",
    "    test_pitch = [np.load(f'{DATA_PATH}pitch/{id}.npy') for id in test_ids[:samples]]\n",
    "    for i, (gt, mel, pitch) in enumerate(zip(ground_truth, test_mels, test_pitch)) :\n",
    "        #plot_spec(mel)\n",
    "        print('\\nGenerating: %i/%i' % (i+1, samples))\n",
    "        gt = 2 * gt.astype(np.float32) / (2**bits - 1.) - 1.\n",
    "        librosa.output.write_wav(f'{GEN_PATH}{k}k_steps_{i}_target.wav', gt, sr=sample_rate)\n",
    "        if batched :\n",
    "            save_str = f'{GEN_PATH}{k}k_steps_{i}_gen_batched_target{target}_overlap{overlap}.wav'\n",
    "        else :    \n",
    "            save_str = f'{GEN_PATH}{k}k_steps_{i}_gen_not_batched.wav'\n",
    "        out = model.generate(mel, pitch, save_str, batched, target, overlap)\n",
    "        print(\"\")\n",
    "        print(mel.shape)\n",
    "        print(gt.shape)\n",
    "        print(out.shape)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    for inpt,output in zip(ground_truth, outputs):\n",
    "        if plot_input:\n",
    "            plot(inpt)\n",
    "        if plot_output:\n",
    "            plot(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now in 3k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.0 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.3 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 3.282 -- 0.52 steps/sec -- Step: 3k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 3.228 -- 0.50 steps/sec -- Step: 3k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 3.196 -- 0.53 steps/sec -- Step: 3k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 3.233 -- 0.52 steps/sec -- Step: 3k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 3.191 -- 0.53 steps/sec -- Step: 3k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 3.183 -- 0.53 steps/sec -- Step: 3k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 3.174 -- 0.53 steps/sec -- Step: 3k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 3.157 -- 0.53 steps/sec -- Step: 3k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 3.153 -- 0.54 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 3.188 -- 0.54 steps/sec -- Step: 4k  <saved>\n",
      "\n",
      "Now in 4k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.3 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 3.081 -- 0.54 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 3.107 -- 0.52 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 3.013 -- 0.52 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 3.075 -- 0.50 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 3.084 -- 0.52 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 3.092 -- 0.51 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 3.026 -- 0.53 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 3.031 -- 0.53 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.982 -- 0.52 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.972 -- 0.52 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.979 -- 0.53 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.967 -- 0.53 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.932 -- 0.53 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.875 -- 0.52 steps/sec -- Step: 4k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.924 -- 0.53 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.832 -- 0.53 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.897 -- 0.53 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.822 -- 0.53 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.792 -- 0.53 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.828 -- 0.52 steps/sec -- Step: 5k  <saved>\n",
      "\n",
      "Now in 5k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.9 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.0 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.767 -- 0.53 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.740 -- 0.52 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.759 -- 0.53 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.725 -- 0.51 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.837 -- 0.53 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.677 -- 0.53 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.627 -- 0.52 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.584 -- 0.53 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.618 -- 0.52 steps/sec -- Step: 5k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.588 -- 0.53 steps/sec -- Step: 6k  <saved>\n",
      "\n",
      "Now in 6k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.5 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.4 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.595 -- 0.53 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.562 -- 0.54 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.542 -- 0.53 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.587 -- 0.53 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.554 -- 0.53 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.505 -- 0.53 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.557 -- 0.53 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.540 -- 0.52 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.515 -- 0.50 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.572 -- 0.51 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.461 -- 0.51 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.426 -- 0.51 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.449 -- 0.51 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.436 -- 0.52 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.481 -- 0.51 steps/sec -- Step: 6k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.501 -- 0.52 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.441 -- 0.49 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.387 -- 0.50 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.558 -- 0.52 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.444 -- 0.53 steps/sec -- Step: 7k  <saved>\n",
      "\n",
      "Now in 7k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.0 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.3 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.6 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.424 -- 0.53 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.426 -- 0.54 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.539 -- 0.54 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.413 -- 0.52 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.423 -- 0.53 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.356 -- 0.52 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.508 -- 0.52 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.329 -- 0.53 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.374 -- 0.53 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.334 -- 0.54 steps/sec -- Step: 7k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.357 -- 0.54 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.400 -- 0.53 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.344 -- 0.52 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.375 -- 0.52 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.394 -- 0.53 steps/sec -- Step: 8k  <saved>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.406 -- 0.54 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.332 -- 0.53 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.305 -- 0.53 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.358 -- 0.54 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.276 -- 0.54 steps/sec -- Step: 8k  <saved>\n",
      "\n",
      "Now in 8k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.9 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.0 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.313 -- 0.54 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.341 -- 0.54 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.349 -- 0.54 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.272 -- 0.54 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.315 -- 0.53 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.329 -- 0.54 steps/sec -- Step: 8k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.373 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.267 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.295 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.328 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "\n",
      "Now in 9k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.276 -- 0.53 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.352 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.531 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.269 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.240 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.291 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.230 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.293 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.261 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.232 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.240 -- 0.54 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.288 -- 0.53 steps/sec -- Step: 9k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.252 -- 0.54 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.290 -- 0.54 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.284 -- 0.54 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.193 -- 0.54 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.200 -- 0.02 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.395 -- 0.54 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.272 -- 0.55 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.258 -- 0.54 steps/sec -- Step: 10k  <saved>\n",
      "\n",
      "Now in 10k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.0 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.4 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 5.3 kHz -- x_realtime: 0.2  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.400 -- 0.54 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.405 -- 0.50 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.272 -- 0.54 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.286 -- 0.55 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.241 -- 0.55 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.231 -- 0.54 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.211 -- 0.54 steps/sec -- Step: 10k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.325 -- 0.54 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.279 -- 0.54 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.221 -- 0.54 steps/sec -- Step: 11k  <saved>\n",
      "\n",
      "Now in 11k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.254 -- 0.54 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.198 -- 0.55 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.221 -- 0.54 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.193 -- 0.52 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.334 -- 0.53 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.242 -- 0.52 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.251 -- 0.53 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.227 -- 0.54 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.229 -- 0.54 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.297 -- 0.55 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.275 -- 0.54 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.155 -- 0.54 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.196 -- 0.54 steps/sec -- Step: 11k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.183 -- 0.54 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.229 -- 0.54 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.179 -- 0.54 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.180 -- 0.55 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.230 -- 0.55 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.181 -- 0.55 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.286 -- 0.55 steps/sec -- Step: 12k  <saved>\n",
      "\n",
      "Now in 12k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.6 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.5 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.197 -- 0.55 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.179 -- 0.55 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.201 -- 0.53 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.128 -- 0.41 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.180 -- 0.01 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.248 -- 0.51 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.142 -- 0.56 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.216 -- 0.57 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.150 -- 0.58 steps/sec -- Step: 12k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.250 -- 0.58 steps/sec -- Step: 13k  <saved>\n",
      "\n",
      "Now in 13k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.5 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.6 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.192 -- 0.58 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.163 -- 0.57 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.164 -- 0.57 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.166 -- 0.57 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.185 -- 0.57 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.164 -- 0.56 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.184 -- 0.57 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.178 -- 0.57 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.146 -- 0.57 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.134 -- 0.57 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.293 -- 0.57 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.140 -- 0.57 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.120 -- 0.57 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.128 -- 0.58 steps/sec -- Step: 13k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.151 -- 0.57 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.150 -- 0.54 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.162 -- 0.56 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.136 -- 0.56 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.139 -- 0.56 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.126 -- 0.56 steps/sec -- Step: 14k  <saved>\n",
      "\n",
      "Now in 14k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.5 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.6 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.5 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.104 -- 0.56 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.102 -- 0.56 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 3/10 -- Batch: 14/64 -- Loss: 2.144 -- 0.58 steps/sec -- Step: 14k \n",
      "Error\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.144 -- 0.01 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.121 -- 0.53 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.173 -- 0.53 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.156 -- 0.53 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.107 -- 0.54 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.236 -- 0.53 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.127 -- 0.54 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.080 -- 0.54 steps/sec -- Step: 14k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.075 -- 0.50 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.066 -- 0.52 steps/sec -- Step: 15k  <saved>\n",
      "\n",
      "Now in 15k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 5.8 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.4 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.127 -- 0.55 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.234 -- 0.54 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.130 -- 0.54 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.127 -- 0.55 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.081 -- 0.54 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.055 -- 0.54 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.072 -- 0.55 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.100 -- 0.55 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.094 -- 0.55 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.048 -- 0.55 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.126 -- 0.55 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.061 -- 0.55 steps/sec -- Step: 15k  <saved>\n",
      "Epoch: 3/10 -- Batch: 59/64 -- Loss: 2.033 -- 0.55 steps/sec -- Step: 15k \n",
      "Error\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.198 -- 0.55 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.069 -- 0.54 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.052 -- 0.54 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.193 -- 0.55 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 5/10 -- Batch: 56/64 -- Loss: 2.076 -- 0.55 steps/sec -- Step: 16k \n",
      "Error\n",
      "\n",
      "Now in 16k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.4 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.284 -- 0.55 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.065 -- 0.54 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.024 -- 0.54 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.034 -- 0.55 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.053 -- 0.55 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.135 -- 0.55 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.151 -- 0.54 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.998 -- 0.55 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.058 -- 0.54 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.105 -- 0.53 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.021 -- 0.53 steps/sec -- Step: 16k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.041 -- 0.54 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.130 -- 0.54 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.103 -- 0.54 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.220 -- 0.54 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.073 -- 0.55 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.991 -- 0.55 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.992 -- 0.55 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.035 -- 0.55 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.152 -- 0.54 steps/sec -- Step: 17k  <saved>\n",
      "\n",
      "Now in 17k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.3 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.6 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.0 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.044 -- 0.54 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.025 -- 0.55 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.036 -- 0.54 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.068 -- 0.54 steps/sec -- Step: 17k  <saved>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.010 -- 0.54 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.994 -- 0.54 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.997 -- 0.54 steps/sec -- Step: 17k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.998 -- 0.54 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.323 -- 0.54 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.149 -- 0.54 steps/sec -- Step: 18k  <saved>\n",
      "\n",
      "Now in 18k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.4 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.3 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.034 -- 0.55 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.002 -- 0.54 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.008 -- 0.55 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.009 -- 0.54 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.098 -- 0.54 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.059 -- 0.55 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.998 -- 0.55 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.960 -- 0.54 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.976 -- 0.53 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.965 -- 0.53 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.110 -- 0.54 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.011 -- 0.54 steps/sec -- Step: 18k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.970 -- 0.54 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.987 -- 0.55 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.954 -- 0.55 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.000 -- 0.54 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.931 -- 0.55 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.100 -- 0.54 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.926 -- 0.55 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.985 -- 0.54 steps/sec -- Step: 19k  <saved>\n",
      "\n",
      "Now in 19k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.8 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.6 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.090 -- 0.54 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 2.092 -- 0.54 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.997 -- 0.54 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.960 -- 0.54 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.034 -- 0.54 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.969 -- 0.54 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.102 -- 0.54 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.954 -- 0.55 steps/sec -- Step: 19k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.963 -- 0.55 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.949 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "\n",
      "Now in 20k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.6 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.3 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.016 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.995 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.250 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.966 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.942 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.901 -- 0.53 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.080 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.942 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.903 -- 0.53 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.115 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.993 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.996 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.926 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.009 -- 0.54 steps/sec -- Step: 20k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.942 -- 0.54 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.965 -- 0.55 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.019 -- 0.54 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.104 -- 0.53 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.919 -- 0.54 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.988 -- 0.53 steps/sec -- Step: 21k  <saved>\n",
      "\n",
      "Now in 21k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.7 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.4 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.964 -- 0.54 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.953 -- 0.54 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.996 -- 0.54 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.962 -- 0.54 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.949 -- 0.54 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.949 -- 0.54 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.913 -- 0.55 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.997 -- 0.54 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.938 -- 0.54 steps/sec -- Step: 21k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.994 -- 0.54 steps/sec -- Step: 22k  <saved>\n",
      "\n",
      "Now in 22k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.1 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.4 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 2.153 -- 0.54 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.892 -- 0.55 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.968 -- 0.54 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.916 -- 0.54 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.936 -- 0.54 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.939 -- 0.55 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 7/10 -- Batch: 26/64 -- Loss: 1.978 -- 0.52 steps/sec -- Step: 22k \n",
      "Error\n",
      "Epoch: 1/10 -- Batch: 34/64 -- Loss: 1.928 -- 0.54 steps/sec -- Step: 22k \n",
      "Error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.933 -- 0.54 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.955 -- 0.54 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.091 -- 0.54 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.023 -- 0.54 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.972 -- 0.54 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.928 -- 0.54 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.031 -- 0.55 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.874 -- 0.54 steps/sec -- Step: 22k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.916 -- 0.55 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.930 -- 0.55 steps/sec -- Step: 23k  <saved>\n",
      "\n",
      "Now in 23k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.1 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.0 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.901 -- 0.55 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.905 -- 0.55 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.008 -- 0.55 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.984 -- 0.54 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.942 -- 0.54 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.987 -- 0.54 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.923 -- 0.54 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.959 -- 0.54 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.933 -- 0.54 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.866 -- 0.55 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.918 -- 0.55 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.902 -- 0.54 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.906 -- 0.55 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.901 -- 0.55 steps/sec -- Step: 23k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.869 -- 0.55 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.916 -- 0.55 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.033 -- 0.54 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.951 -- 0.53 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 9/10 -- Batch: 1/64 -- Loss: 1.715 -- 0.55 steps/sec -- Step: 24k \n",
      "Error\n",
      "\n",
      "Now in 24k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.3 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.916 -- 0.55 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.886 -- 0.54 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.886 -- 0.54 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.000 -- 0.54 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.010 -- 0.54 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.962 -- 0.55 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.936 -- 0.55 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.884 -- 0.55 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.893 -- 0.55 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.956 -- 0.55 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.887 -- 0.55 steps/sec -- Step: 24k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.917 -- 0.54 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.949 -- 0.53 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.890 -- 0.53 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.871 -- 0.55 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.004 -- 0.54 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.881 -- 0.54 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.865 -- 0.54 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.907 -- 0.54 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.883 -- 0.54 steps/sec -- Step: 25k  <saved>\n",
      "\n",
      "Now in 25k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.0 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.1 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.923 -- 0.55 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.916 -- 0.55 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.197 -- 0.54 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.933 -- 0.54 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.882 -- 0.54 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 2.003 -- 0.54 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.841 -- 0.54 steps/sec -- Step: 25k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.922 -- 0.54 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 2.069 -- 0.54 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.844 -- 0.55 steps/sec -- Step: 26k  <saved>\n",
      "\n",
      "Now in 26k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.4 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.877 -- 0.54 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.839 -- 0.55 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.968 -- 0.54 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.852 -- 0.54 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.864 -- 0.55 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.926 -- 0.55 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.825 -- 0.55 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.870 -- 0.54 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.976 -- 0.55 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.943 -- 0.55 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.864 -- 0.54 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.937 -- 0.55 steps/sec -- Step: 26k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.913 -- 0.55 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.878 -- 0.55 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.974 -- 0.55 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.942 -- 0.54 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.847 -- 0.54 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.864 -- 0.55 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.934 -- 0.55 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.843 -- 0.55 steps/sec -- Step: 27k  <saved>\n",
      "\n",
      "Now in 27k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.3 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.6 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.4 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.911 -- 0.55 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.943 -- 0.54 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.880 -- 0.54 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.830 -- 0.55 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.941 -- 0.55 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.868 -- 0.55 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.928 -- 0.55 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.873 -- 0.55 steps/sec -- Step: 27k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.854 -- 0.54 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.880 -- 0.54 steps/sec -- Step: 28k  <saved>\n",
      "\n",
      "Now in 28k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.3 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.3 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.3 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.945 -- 0.54 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.882 -- 0.54 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.843 -- 0.55 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.854 -- 0.54 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.844 -- 0.54 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.984 -- 0.54 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.872 -- 0.55 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.897 -- 0.54 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.926 -- 0.54 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.991 -- 0.54 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.840 -- 0.55 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.973 -- 0.55 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.847 -- 0.55 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.845 -- 0.55 steps/sec -- Step: 28k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.885 -- 0.54 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.910 -- 0.55 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.858 -- 0.55 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.964 -- 0.54 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.846 -- 0.54 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.836 -- 0.55 steps/sec -- Step: 29k  <saved>\n",
      "\n",
      "Now in 29k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.0 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.0 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.819 -- 0.54 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.926 -- 0.54 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.094 -- 0.54 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.872 -- 0.55 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.824 -- 0.54 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.837 -- 0.55 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.834 -- 0.55 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 2.026 -- 0.54 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.885 -- 0.54 steps/sec -- Step: 29k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.864 -- 0.53 steps/sec -- Step: 30k  <saved>\n",
      "\n",
      "Now in 30k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.9 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.842 -- 0.55 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.860 -- 0.55 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 2.005 -- 0.55 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.880 -- 0.55 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.836 -- 0.54 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.867 -- 0.54 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.909 -- 0.54 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.992 -- 0.54 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.862 -- 0.54 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.857 -- 0.54 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.818 -- 0.54 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.851 -- 0.54 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.927 -- 0.54 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.901 -- 0.54 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.804 -- 0.54 steps/sec -- Step: 30k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.831 -- 0.54 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.844 -- 0.55 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.926 -- 0.55 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.910 -- 0.54 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.802 -- 0.54 steps/sec -- Step: 31k  <saved>\n",
      "\n",
      "Now in 31k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.4 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.4 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.864 -- 0.55 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.854 -- 0.55 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.834 -- 0.54 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.848 -- 0.54 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.846 -- 0.54 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.880 -- 0.53 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.843 -- 0.54 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.804 -- 0.55 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.918 -- 0.54 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.862 -- 0.54 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.812 -- 0.54 steps/sec -- Step: 31k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.868 -- 0.54 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.811 -- 0.55 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.826 -- 0.55 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 2.071 -- 0.55 steps/sec -- Step: 32k  <saved>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.810 -- 0.55 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.823 -- 0.55 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.838 -- 0.55 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.912 -- 0.55 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.008 -- 0.55 steps/sec -- Step: 32k  <saved>\n",
      "\n",
      "Now in 32k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.6 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.3 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.768 -- 0.55 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.803 -- 0.54 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 3/10 -- Batch: 10/64 -- Loss: 1.789 -- 0.54 steps/sec -- Step: 32k \n",
      "Error\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.844 -- 0.54 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.797 -- 0.54 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.925 -- 0.55 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.784 -- 0.55 steps/sec -- Step: 32k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.826 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.802 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 2.036 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.859 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.827 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.829 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "\n",
      "Now in 33k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.9 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.4 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.844 -- 0.55 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.848 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.895 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.845 -- 0.53 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.787 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.819 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.811 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.787 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.788 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.822 -- 0.54 steps/sec -- Step: 33k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.856 -- 0.54 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.838 -- 0.55 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.966 -- 0.54 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.912 -- 0.54 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.815 -- 0.55 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.878 -- 0.54 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.783 -- 0.55 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.789 -- 0.55 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.841 -- 0.54 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.839 -- 0.54 steps/sec -- Step: 34k  <saved>\n",
      "\n",
      "Now in 34k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.0 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.4 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.939 -- 0.54 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.797 -- 0.54 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.792 -- 0.53 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.105 -- 0.54 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.891 -- 0.54 steps/sec -- Step: 34k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.839 -- 0.54 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.838 -- 0.54 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.831 -- 0.54 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.872 -- 0.55 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.850 -- 0.54 steps/sec -- Step: 35k  <saved>\n",
      "\n",
      "Now in 35k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.1 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.2 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.843 -- 0.52 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.763 -- 0.54 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.844 -- 0.53 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.812 -- 0.54 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.812 -- 0.54 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.901 -- 0.55 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.752 -- 0.55 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.843 -- 0.55 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.731 -- 0.55 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.840 -- 0.55 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.831 -- 0.55 steps/sec -- Step: 35k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.775 -- 0.55 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.811 -- 0.52 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.946 -- 0.53 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.836 -- 0.54 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.812 -- 0.54 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.855 -- 0.55 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.802 -- 0.55 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.840 -- 0.55 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.812 -- 0.54 steps/sec -- Step: 36k  <saved>\n",
      "\n",
      "Now in 36k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.9 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.6 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.4 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.819 -- 0.55 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.829 -- 0.55 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.874 -- 0.55 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.948 -- 0.55 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.797 -- 0.55 steps/sec -- Step: 36k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.790 -- 0.55 steps/sec -- Step: 36k  <saved>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.992 -- 0.55 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.788 -- 0.55 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.781 -- 0.54 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.879 -- 0.55 steps/sec -- Step: 37k  <saved>\n",
      "\n",
      "Now in 37k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.4 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.5 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 7.3 kHz -- x_realtime: 0.3  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.834 -- 0.55 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.817 -- 0.53 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.873 -- 0.54 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 2.005 -- 0.54 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.810 -- 0.52 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.791 -- 0.52 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.791 -- 0.54 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.771 -- 0.52 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.921 -- 0.52 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 1.772 -- 0.53 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.916 -- 0.54 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.880 -- 0.51 steps/sec -- Step: 37k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.755 -- 0.52 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.980 -- 0.53 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.833 -- 0.53 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.781 -- 0.53 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.821 -- 0.54 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.863 -- 0.52 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.804 -- 0.53 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 10/10 -- Batch: 64/64 -- Loss: 2.106 -- 0.53 steps/sec -- Step: 38k  <saved>\n",
      "\n",
      "Now in 38k steps\n",
      "Generating outputs\n",
      "\n",
      "Generating: 1/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 6.5 kHz -- x_realtime: 0.3  \n",
      "(13, 1819)\n",
      "(212893,)\n",
      "(220000,)\n",
      "\n",
      "Generating: 2/3\n",
      "48000/48400 -- batch_size: 4 -- gen_rate: 1.2 kHz -- x_realtime: 0.1  \n",
      "(13, 358)\n",
      "(41885,)\n",
      "(46750,)\n",
      "\n",
      "Generating: 3/3\n",
      "228000/229900 -- batch_size: 19 -- gen_rate: 5.5 kHz -- x_realtime: 0.2  \n",
      "(13, 1821)\n",
      "(213149,)\n",
      "(220000,)\n",
      "Epoch: 1/10 -- Batch: 64/64 -- Loss: 1.836 -- 0.53 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 2/10 -- Batch: 64/64 -- Loss: 1.774 -- 0.52 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 3/10 -- Batch: 64/64 -- Loss: 1.876 -- 0.53 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 4/10 -- Batch: 64/64 -- Loss: 1.961 -- 0.53 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 5/10 -- Batch: 64/64 -- Loss: 1.962 -- 0.54 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 6/10 -- Batch: 64/64 -- Loss: 1.764 -- 0.54 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 7/10 -- Batch: 64/64 -- Loss: 1.857 -- 0.52 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 8/10 -- Batch: 64/64 -- Loss: 1.844 -- 0.50 steps/sec -- Step: 38k  <saved>\n",
      "Epoch: 9/10 -- Batch: 64/64 -- Loss: 1.853 -- 0.48 steps/sec -- Step: 39k  <saved>\n",
      "Epoch: 10/10 -- Batch: 9/64 -- Loss: 1.803 -- 0.50 steps/sec -- Step: 39k "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        if step // 1000 != current_thousand:\n",
    "            current_thousand = step // 1000\n",
    "            print(f'\\nNow in {current_thousand}k steps\\nGenerating outputs')\n",
    "            generate(3,batched=True,plot_output=False)\n",
    "        train(model, optimiser, epochs=10, batch_size=32, classes=2**bits, \n",
    "                seq_len=seq_len, step=step, lr=1e-4)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except:\n",
    "        print(\"\\nError\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
